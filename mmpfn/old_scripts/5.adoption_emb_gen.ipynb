{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from math import ceil\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, root_mean_squared_error\n",
    "\n",
    "from mmpfn.models.mmpfn_v2 import MMPFNClassifier\n",
    "from mmpfn.models.dino_v2.models.vision_transformer import vit_base\n",
    "from mmpfn.models.mmpfn_v2.constants import ModelInterfaceConfig\n",
    "from mmpfn.models.mmpfn_v2.preprocessing import PreprocessorConfig\n",
    "from mmpfn.scripts_finetune_mm.finetune_tabpfn_main import fine_tune_tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3739a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_features = [\"Age\",\"Breed1\",\"Breed2\",\"Color1\",\"Color2\",\"Color3\",\"Dewormed\",\"Fee\",\"FurLength\",\"Gender\",\"Health\",\"MaturitySize\",\"PhotoAmt\",\"State\",\"Sterilized\",\"Type\",\"Vaccinated\",\"VideoAmt\",\"Quantity\",]\n",
    "col_exclude = [\"PetID\", \"RescureID\", \"Description\", \"Name\"]\n",
    "col_target = \"AdoptionSpeed\"\n",
    "cat_features = [\"Breed1\",\"Breed2\",\"Color1\",\"Color2\",\"Color3\",\"Dewormed\",\"FurLength\",\"Gender\",\"Health\",\"MaturitySize\",\"State\",\"Sterilized\",\"Type\",\"Vaccinated\",]\n",
    "cat_features_index = [col_features.index(feature) for feature in cat_features]\n",
    "train = pd.read_csv(\"datasets/petfinder-adoption-prediction/train/train.csv\")\n",
    "datasets_dir = \"datasets/petfinder-adoption-prediction\"\n",
    "\n",
    "train[\"PetID\"] = train[\"PetID\"].astype(str)\n",
    "train_images = [f for f in os.listdir(os.path.join(datasets_dir, \"train_images\")) if f.endswith(\".jpg\")]\n",
    "train_images = [f for f in train_images if f.split(\"-\")[0] in train[\"PetID\"].values]\n",
    "train_images_df = pd.DataFrame(\n",
    "    {\n",
    "        \"PetID\": [f.split(\"-\")[0] for f in train_images],\n",
    "        \"ImageNumber\": [f.split(\"-\")[1].split(\".\")[0] for f in train_images],\n",
    "    }\n",
    ")\n",
    "train_images_df = train_images_df[train_images_df[\"ImageNumber\"] == \"1\"]\n",
    "train = train.merge(train_images_df, on=\"PetID\", how=\"left\")\n",
    "train = train[train[\"ImageNumber\"].notna()]\n",
    "train[\"ImagePath\"] = train[\"PetID\"] + \"-1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000face1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_patch = 'adoption_patch.pt'\n",
    "path_cls = 'adoption_cls.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_cls):\n",
    "    # adoption_patch = torch.load(path_patch)\n",
    "    # adoption_cls = torch.load(path_cls)\n",
    "    pass\n",
    "else:\n",
    "    img_size = 14*24\n",
    "    X_image = []\n",
    "    i = 0\n",
    "    for path in train['ImagePath']:\n",
    "        full_path = os.path.join(datasets_dir, \"train_images\", path)\n",
    "        if not os.path.exists(full_path):\n",
    "            print(f\"Image {full_path} does not exist, skipping.\")\n",
    "            break\n",
    "        with Image.open(full_path) as img:\n",
    "            if img.mode == \"L\":\n",
    "                img = img.convert(\"RGB\")\n",
    "            image = np.array(img.resize((img_size, img_size), Image.BILINEAR))\n",
    "        X_image.append(image)\n",
    "    X_image = np.array(X_image)\n",
    "    \n",
    "    image_encoder = vit_base(\n",
    "        patch_size=14, img_size=518, init_values=1.0, num_register_tokens=0, block_chunks=0\n",
    "    )\n",
    "\n",
    "    image_model_path = f\"{Path().absolute()}/parameters/dinov2_vitb14_pretrain.pth\"\n",
    "    image_state_dict = torch.load(image_model_path)\n",
    "    image_encoder.load_state_dict(image_state_dict)\n",
    "    _ = image_encoder.cuda().eval()\n",
    "\n",
    "    batch_size = 16\n",
    "    adoption_patch, adoption_cls = [], []\n",
    "\n",
    "    X_image_torch = torch.from_numpy(\n",
    "    np.transpose(X_image, (0,3,1,2))\n",
    "    ).float()\n",
    "\n",
    "    X_image_torch /= 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, X_image_torch.shape[0], batch_size):\n",
    "            batch = X_image_torch[i:i+batch_size].to(\"cuda\", non_blocking=True)\n",
    "            feats = image_encoder.forward_features(batch)\n",
    "            adoption_patch.append(feats['x_norm_patchtokens'].detach().cpu())\n",
    "            adoption_cls.append(feats['x_norm_clstoken'].detach().cpu())\n",
    "\n",
    "    # adoption_patch = [x.detach().cpu() for x in adoption_patch]\n",
    "    # adoption_patch = torch.cat(adoption_patch, dim=0)\n",
    "    # torch.save(adoption_patch.cpu(), path_patch)\n",
    "    torch.save(adoption_cls, path_cls)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.save(adoption_patch, path_patch)\n",
    "    \n",
    "    # adoption_cls = [x.detach().cpu() for x in adoption_cls]\n",
    "    # adoption_cls = torch.cat(adoption_cls, dim=0)\n",
    "    # torch.save(adoption_cls.cpu(), path_cls)\n",
    "    \n",
    "    \n",
    "    # print(adoption_patch.shape, adoption_cls.shape)\n",
    "    # torch.save(adoption_cls.detach().cpu(), path_cls)\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    # for i, part in enumerate(adoption_patch.split(1000)):  # split along dim=0\n",
    "    #     torch.save(part.cpu(), f\"adoption_patch_part{i}.pt\")\n",
    "\n",
    "    # # for i, part in enumerate(adoption_cls.split(1000)):  # split along dim=0\n",
    "    # #     torch.save(part.cpu(), f\"adoption_cls_part{i}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dfce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
