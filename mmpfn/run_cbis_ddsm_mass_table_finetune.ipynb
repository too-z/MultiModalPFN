{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8292ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os \n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from mmpfn.datasets.cbis_ddsm import CBISDDSMDataset\n",
    "\n",
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, root_mean_squared_error\n",
    "\n",
    "from mmpfn.models.tabpfn_v2 import TabPFNClassifier\n",
    "from mmpfn.models.dino_v2.models.vision_transformer import vit_base\n",
    "from mmpfn.models.tabpfn_v2.constants import ModelInterfaceConfig\n",
    "from mmpfn.models.tabpfn_v2.preprocessing import PreprocessorConfig\n",
    "from mmpfn.scripts_finetune.finetune_tabpfn_main import fine_tune_tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd5c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.path.join(os.getenv('HOME'), \"workspace/works/tabular_image/MultiModalPFN/mmpfn/data/cbis_ddsm\")\n",
    "data_path = os.path.join(os.getenv('HOME'), \"works/research/MultiModalPFN/mmpfn/data/cbis_ddsm\")\n",
    "\n",
    "kind = 'mass'  # mass calc\n",
    "image_type = 'all' # all full crop roi\n",
    "test_dataset = CBISDDSMDataset(data_path=data_path, data_name=f'csv/{kind}_case_description_test_set.csv', kind=kind, image_type=image_type)\n",
    "# _ = test_dataset.get_images()\n",
    "# _ = test_dataset.get_embeddings(mode='test')\n",
    "train_dataset = CBISDDSMDataset(data_path=data_path, data_name=f'csv/{kind}_case_description_train_set.csv', kind=kind, image_type=image_type)\n",
    "# _ = train_dataset.get_images()\n",
    "# _ = train_dataset.get_embeddings(mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0144ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 23:52:28,965] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  23%|██▎       | 23/100 [00:06<00:18,  4.06it/s, Best Val. Loss=0.276, Best Val. Score=-0.276, Training Loss=0.247, Val. Loss=0.276, Patience=29, Utilization=0, Grad Norm=3.76][2025-09-17 23:52:35,305] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:24,  4.11it/s, Best Val. Loss=0.249, Best Val. Score=-0.249, Training Loss=0.287, Val. Loss=0.259, Patience=-48, Utilization=0, Grad Norm=3.81]                         \n",
      "[2025-09-17 23:52:52,870] INFO - Initial Validation Loss: 0.6337409752581222 Best Validation Loss: 0.2490369253392985 Total Steps: 101 Best Step: 68 Total Time Spent: 25.512139320373535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 23:52:53,726] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  23%|██▎       | 23/100 [00:05<00:21,  3.59it/s, Best Val. Loss=0.275, Best Val. Score=-0.275, Training Loss=0.247, Val. Loss=0.275, Patience=29, Utilization=0, Grad Norm=3.76][2025-09-17 23:52:59,415] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:24,  4.12it/s, Best Val. Loss=0.249, Best Val. Score=-0.249, Training Loss=0.284, Val. Loss=0.259, Patience=-48, Utilization=0, Grad Norm=4.16]                         \n",
      "[2025-09-17 23:53:17,796] INFO - Initial Validation Loss: 0.6337409752581222 Best Validation Loss: 0.24926094292862372 Total Steps: 101 Best Step: 68 Total Time Spent: 24.51719880104065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 23:53:18,516] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  23%|██▎       | 23/100 [00:05<00:20,  3.76it/s, Best Val. Loss=0.276, Best Val. Score=-0.276, Training Loss=0.247, Val. Loss=0.276, Patience=29, Utilization=0, Grad Norm=3.74][2025-09-17 23:53:24,270] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:24,  4.05it/s, Best Val. Loss=0.248, Best Val. Score=-0.248, Training Loss=0.282, Val. Loss=0.258, Patience=-48, Utilization=0, Grad Norm=3.77]                         \n",
      "[2025-09-17 23:53:43,010] INFO - Initial Validation Loss: 0.6337409752581222 Best Validation Loss: 0.24790622825402506 Total Steps: 101 Best Step: 67 Total Time Spent: 24.916123390197754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.708994708994709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 23:53:43,738] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  23%|██▎       | 23/100 [00:05<00:21,  3.66it/s, Best Val. Loss=0.276, Best Val. Score=-0.276, Training Loss=0.247, Val. Loss=0.276, Patience=29, Utilization=0, Grad Norm=3.75][2025-09-17 23:53:49,410] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:24,  4.08it/s, Best Val. Loss=0.248, Best Val. Score=-0.248, Training Loss=0.285, Val. Loss=0.258, Patience=-48, Utilization=0, Grad Norm=3.67]                         \n",
      "[2025-09-17 23:54:08,027] INFO - Initial Validation Loss: 0.6337409752581222 Best Validation Loss: 0.24809013480246075 Total Steps: 101 Best Step: 69 Total Time Spent: 24.712132453918457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 23:54:08,784] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  23%|██▎       | 23/100 [00:06<00:21,  3.65it/s, Best Val. Loss=0.275, Best Val. Score=-0.275, Training Loss=0.247, Val. Loss=0.275, Patience=29, Utilization=0, Grad Norm=3.74][2025-09-17 23:54:14,816] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:25,  3.96it/s, Best Val. Loss=0.249, Best Val. Score=-0.249, Training Loss=0.286, Val. Loss=0.257, Patience=-48, Utilization=0, Grad Norm=3.78]                         \n",
      "[2025-09-17 23:54:33,808] INFO - Initial Validation Loss: 0.6337409752581222 Best Validation Loss: 0.24939014855316755 Total Steps: 101 Best Step: 70 Total Time Spent: 25.47745132446289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.7116402116402116\n"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # print(f\"Finetuning with seed: {seed}\")\n",
    "    \n",
    "    X_train = train_dataset.x\n",
    "    y_train = train_dataset.y\n",
    "    X_test = test_dataset.x\n",
    "    y_test = test_dataset.y\n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        col = X_train[:, i]\n",
    "        col[np.isnan(col)] = np.nanmin(col) - 1\n",
    "    for i in range(X_test.shape[1]):\n",
    "        col = X_test[:, i]\n",
    "        col[np.isnan(col)] = np.nanmin(col) - 1\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    save_path_to_fine_tuned_model = \"./finetuned_tabpfn_cbis_mass.ckpt\"\n",
    "    \n",
    "    fine_tune_tabpfn(\n",
    "        # path_to_base_model=\"auto\",\n",
    "        save_path_to_fine_tuned_model=save_path_to_fine_tuned_model,\n",
    "        # Finetuning HPs\n",
    "        time_limit=60,\n",
    "        finetuning_config={\"learning_rate\": 0.00001, \"batch_size\": 1, \"max_steps\": 100},\n",
    "        validation_metric=\"log_loss\",\n",
    "        # Input Data\n",
    "        X_train=pd.DataFrame(X_train),\n",
    "        y_train=pd.Series(y_train),\n",
    "        categorical_features_index=None,\n",
    "        device=\"cuda\",  # use \"cpu\" if you don't have a GPU\n",
    "        task_type=\"binary\",\n",
    "        # Optional\n",
    "        show_training_curve=False,  # Shows a final report after finetuning.\n",
    "        logger_level=0,  # Shows all logs, higher values shows less\n",
    "    )\n",
    "\n",
    "    # disables preprocessing at inference time to match fine-tuning\n",
    "    no_preprocessing_inference_config = ModelInterfaceConfig(\n",
    "        FINGERPRINT_FEATURE=False,\n",
    "        PREPROCESS_TRANSFORMS=[PreprocessorConfig(name='none')]\n",
    "    )\n",
    "\n",
    "    # Evaluate on Test Data\n",
    "    model_finetuned = TabPFNClassifier(\n",
    "        model_path=save_path_to_fine_tuned_model,\n",
    "        inference_config=no_preprocessing_inference_config,\n",
    "        ignore_pretraining_limits=True,\n",
    "    )\n",
    "\n",
    "    clf_finetuned = model_finetuned.fit(X_train, y_train)\n",
    "    acc_score = accuracy_score(y_test, clf_finetuned.predict(X_test))\n",
    "    print(\"accuracy_score (Finetuned):\", acc_score)\n",
    "    accuracy_scores.append(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2ba9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7126984126984127\n",
      "Std Accuracy: 0.0021164021164021387\n"
     ]
    }
   ],
   "source": [
    "# get mean and std of accuracy scores\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Std Accuracy:\", std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dfce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
