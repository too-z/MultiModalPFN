{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os \n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from mmpfn.datasets.pad_ufes_20 import PADUFES20Dataset\n",
    "\n",
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, root_mean_squared_error\n",
    "\n",
    "from mmpfn.models.tabpfn_v2 import TabPFNClassifier\n",
    "from mmpfn.models.dino_v2.models.vision_transformer import vit_base\n",
    "from mmpfn.models.tabpfn_v2.constants import ModelInterfaceConfig\n",
    "from mmpfn.models.tabpfn_v2.preprocessing import PreprocessorConfig\n",
    "from mmpfn.scripts_finetune.finetune_tabpfn_main import fine_tune_tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.path.join(os.getenv('HOME'), \"workspace/works/tabular_image/MultiModalPFN/mmpfn/data/pad_ufes_20\")\n",
    "data_path = os.path.join(os.getenv('HOME'), \"works/research/MultiModalPFN/mmpfn/data/pad_ufes_20\")\n",
    "dataset = PADUFES20Dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-08-20 21:41:01,672] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   7%|▋         | 7/100 [00:02<00:34,  2.70it/s, Best Val. Loss=0.562, Best Val. Score=-0.562, Training Loss=0.437, Val. Loss=0.569, Patience=45, Utilization=0, Grad Norm=5.93][2025-08-20 21:41:03,849] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:36,  2.71it/s, Best Val. Loss=0.527, Best Val. Score=-0.527, Training Loss=0.457, Val. Loss=0.538, Patience=-48, Utilization=0, Grad Norm=4.82]                         \n",
      "[2025-08-20 21:41:38,105] INFO - Initial Validation Loss: 0.565506809631864 Best Validation Loss: 0.5268581387941693 Total Steps: 101 Best Step: 45 Total Time Spent: 38.10961365699768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8434782608695652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   6%|▌         | 6/100 [00:02<00:37,  2.53it/s, Best Val. Loss=0.557, Best Val. Score=-0.557, Training Loss=0.545, Val. Loss=0.557, Patience=45, Utilization=0, Grad Norm=3.87][2025-08-20 21:41:41,212] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  28%|██▊       | 28/100 [00:10<00:26,  2.72it/s, Best Val. Loss=0.533, Best Val. Score=-0.533, Training Loss=0.509, Val. Loss=0.533, Patience=24, Utilization=0, Grad Norm=6.2] [2025-08-20 21:41:49,756] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:36,  2.73it/s, Best Val. Loss=0.533, Best Val. Score=-0.533, Training Loss=0.476, Val. Loss=0.536, Patience=-48, Utilization=0, Grad Norm=5.42]                         \n",
      "[2025-08-20 21:42:15,538] INFO - Initial Validation Loss: 0.591181935659142 Best Validation Loss: 0.5328848436445162 Total Steps: 101 Best Step: 65 Total Time Spent: 36.82858920097351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8217391304347826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   3%|▎         | 3/100 [00:00<00:45,  2.15it/s, Best Val. Loss=0.619, Best Val. Score=-0.619, Training Loss=0.465, Val. Loss=0.619, Patience=48, Utilization=0, Grad Norm=4.09][2025-08-20 21:42:17,525] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  14%|█▍        | 14/100 [00:05<00:34,  2.50it/s, Best Val. Loss=0.579, Best Val. Score=-0.579, Training Loss=0.543, Val. Loss=0.579, Patience=38, Utilization=0, Grad Norm=6.03][2025-08-20 21:42:21,852] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:37,  2.66it/s, Best Val. Loss=0.555, Best Val. Score=-0.555, Training Loss=0.376, Val. Loss=0.566, Patience=-48, Utilization=0, Grad Norm=4.77]                         \n",
      "[2025-08-20 21:42:53,933] INFO - Initial Validation Loss: 0.6289394323962494 Best Validation Loss: 0.554768468077869 Total Steps: 101 Best Step: 60 Total Time Spent: 37.871843099594116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8173913043478261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-08-20 21:42:54,984] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  38%|███▊      | 38/100 [00:13<00:21,  2.82it/s, Best Val. Loss=0.503, Best Val. Score=-0.503, Training Loss=0.513, Val. Loss=0.51, Patience=14, Utilization=0, Grad Norm=5.63] [2025-08-20 21:43:08,380] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:35,  2.79it/s, Best Val. Loss=0.503, Best Val. Score=-0.503, Training Loss=0.549, Val. Loss=0.518, Patience=-48, Utilization=0, Grad Norm=6.1]                          \n",
      "[2025-08-20 21:43:30,519] INFO - Initial Validation Loss: 0.5542218364408179 Best Validation Loss: 0.5028813822545214 Total Steps: 101 Best Step: 6 Total Time Spent: 36.062368392944336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8478260869565217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   6%|▌         | 6/100 [00:01<00:36,  2.57it/s, Best Val. Loss=0.518, Best Val. Score=-0.518, Training Loss=0.521, Val. Loss=0.518, Patience=45, Utilization=0, Grad Norm=2.96][2025-08-20 21:43:33,536] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  15%|█▌        | 15/100 [00:05<00:33,  2.51it/s, Best Val. Loss=0.471, Best Val. Score=-0.471, Training Loss=0.576, Val. Loss=0.471, Patience=37, Utilization=0, Grad Norm=5.73][2025-08-20 21:43:37,027] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:36,  2.73it/s, Best Val. Loss=0.458, Best Val. Score=-0.458, Training Loss=0.387, Val. Loss=0.466, Patience=-48, Utilization=0, Grad Norm=3.84]                         \n",
      "[2025-08-20 21:44:07,915] INFO - Initial Validation Loss: 0.5341567649422696 Best Validation Loss: 0.4580979680446081 Total Steps: 101 Best Step: 52 Total Time Spent: 36.81344962120056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8217391304347826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   2%|▏         | 2/100 [00:00<00:41,  2.36it/s, Best Val. Loss=0.602, Best Val. Score=-0.602, Training Loss=0.552, Val. Loss=0.607, Patience=49, Utilization=0, Grad Norm=3.84][2025-08-20 21:44:09,377] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  53%|█████▎    | 53/100 [00:19<00:18,  2.61it/s, Best Val. Loss=0.559, Best Val. Score=-0.559, Training Loss=0.602, Val. Loss=0.559, Patience=-1, Utilization=0, Grad Norm=6.63][2025-08-20 21:44:28,837] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:37,  2.65it/s, Best Val. Loss=0.551, Best Val. Score=-0.551, Training Loss=0.527, Val. Loss=0.551, Patience=-48, Utilization=0, Grad Norm=4.24]                         \n",
      "[2025-08-20 21:44:46,451] INFO - Initial Validation Loss: 0.602258630216833 Best Validation Loss: 0.5506635703916004 Total Steps: 101 Best Step: 100 Total Time Spent: 38.00792670249939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8565217391304348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   4%|▍         | 4/100 [00:01<00:34,  2.80it/s, Best Val. Loss=0.578, Best Val. Score=-0.578, Training Loss=0.495, Val. Loss=0.579, Patience=47, Utilization=0, Grad Norm=3.84][2025-08-20 21:44:48,578] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  59%|█████▉    | 59/100 [00:20<00:14,  2.85it/s, Best Val. Loss=0.578, Best Val. Score=-0.578, Training Loss=0.515, Val. Loss=0.612, Patience=-7, Utilization=0, Grad Norm=6.64][2025-08-20 21:45:08,231] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:35,  2.80it/s, Best Val. Loss=0.578, Best Val. Score=-0.578, Training Loss=0.554, Val. Loss=0.631, Patience=-48, Utilization=0, Grad Norm=6.64]                         \n",
      "[2025-08-20 21:45:22,898] INFO - Initial Validation Loss: 0.5781870810491588 Best Validation Loss: 0.5781870810491588 Total Steps: 101 Best Step: 0 Total Time Spent: 35.89686703681946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   3%|▎         | 3/100 [00:00<00:37,  2.57it/s, Best Val. Loss=0.587, Best Val. Score=-0.587, Training Loss=0.469, Val. Loss=0.588, Patience=48, Utilization=0, Grad Norm=3.29][2025-08-20 21:45:24,758] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  19%|█▉        | 19/100 [00:06<00:29,  2.77it/s, Best Val. Loss=0.553, Best Val. Score=-0.553, Training Loss=0.612, Val. Loss=0.557, Patience=33, Utilization=0, Grad Norm=5.48][2025-08-20 21:45:30,769] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:36,  2.76it/s, Best Val. Loss=0.552, Best Val. Score=-0.552, Training Loss=0.475, Val. Loss=0.556, Patience=-48, Utilization=0, Grad Norm=6.04]                         \n",
      "[2025-08-20 21:45:59,906] INFO - Initial Validation Loss: 0.59767318805131 Best Validation Loss: 0.5520649972839601 Total Steps: 101 Best Step: 39 Total Time Spent: 36.465503215789795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.8369565217391305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-08-20 21:46:00,938] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  19%|█▉        | 19/100 [00:06<00:31,  2.57it/s, Best Val. Loss=0.53, Best Val. Score=-0.53, Training Loss=0.596, Val. Loss=0.53, Patience=33, Utilization=0, Grad Norm=6.08]   [2025-08-20 21:46:07,866] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:36,  2.74it/s, Best Val. Loss=0.521, Best Val. Score=-0.521, Training Loss=0.397, Val. Loss=0.525, Patience=-48, Utilization=0, Grad Norm=6.07]                         \n",
      "[2025-08-20 21:46:37,122] INFO - Initial Validation Loss: 0.5653504830895202 Best Validation Loss: 0.5206775840861005 Total Steps: 101 Best Step: 50 Total Time Spent: 36.70320010185242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.808695652173913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   5%|▌         | 5/100 [00:01<00:39,  2.42it/s, Best Val. Loss=0.524, Best Val. Score=-0.524, Training Loss=0.477, Val. Loss=0.524, Patience=46, Utilization=0, Grad Norm=2.73][2025-08-20 21:46:39,877] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  30%|███       | 30/100 [00:11<00:27,  2.53it/s, Best Val. Loss=0.475, Best Val. Score=-0.475, Training Loss=0.506, Val. Loss=0.475, Patience=22, Utilization=0, Grad Norm=5.86][2025-08-20 21:46:49,648] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [00:37,  2.69it/s, Best Val. Loss=0.467, Best Val. Score=-0.467, Training Loss=0.562, Val. Loss=0.475, Patience=-48, Utilization=0, Grad Norm=6.4]                          \n",
      "[2025-08-20 21:47:15,101] INFO - Initial Validation Loss: 0.5374249483579934 Best Validation Loss: 0.46681897809388295 Total Steps: 101 Best Step: 40 Total Time Spent: 37.45864796638489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.85\n"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # print(f\"Finetuning with seed: {seed}\")\n",
    "    \n",
    "    train_len = int(len(dataset) * 0.8)\n",
    "    test_len = len(dataset) - train_len\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "    X_train = train_dataset.dataset.x[train_dataset.indices]\n",
    "    y_train = train_dataset.dataset.y[train_dataset.indices]\n",
    "    X_test = test_dataset.dataset.x[test_dataset.indices]\n",
    "    y_test = test_dataset.dataset.y[test_dataset.indices]\n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        col = X_train[:, i]\n",
    "        col[np.isnan(col)] = np.nanmin(col) - 1\n",
    "    for i in range(X_test.shape[1]):\n",
    "        col = X_test[:, i]\n",
    "        col[np.isnan(col)] = np.nanmin(col) - 1\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    save_path_to_fine_tuned_model = \"./finetuned_tabpfn_pad_ufes_20.ckpt\"\n",
    "    \n",
    "    fine_tune_tabpfn(\n",
    "        # path_to_base_model=\"auto\",\n",
    "        save_path_to_fine_tuned_model=save_path_to_fine_tuned_model,\n",
    "        # Finetuning HPs\n",
    "        time_limit=60,\n",
    "        finetuning_config={\"learning_rate\": 0.00001, \"batch_size\": 1, \"max_steps\": 100},\n",
    "        validation_metric=\"log_loss\",\n",
    "        # Input Data\n",
    "        X_train=pd.DataFrame(X_train),\n",
    "        y_train=pd.Series(y_train),\n",
    "        categorical_features_index=None,\n",
    "        device=\"cuda\",  # use \"cpu\" if you don't have a GPU\n",
    "        task_type=\"multiclass\",\n",
    "        # Optional\n",
    "        show_training_curve=False,  # Shows a final report after finetuning.\n",
    "        logger_level=0,  # Shows all logs, higher values shows less\n",
    "    )\n",
    "\n",
    "    # disables preprocessing at inference time to match fine-tuning\n",
    "    no_preprocessing_inference_config = ModelInterfaceConfig(\n",
    "        FINGERPRINT_FEATURE=False,\n",
    "        PREPROCESS_TRANSFORMS=[PreprocessorConfig(name='none')]\n",
    "    )\n",
    "\n",
    "    # Evaluate on Test Data\n",
    "    model_finetuned = TabPFNClassifier(\n",
    "        model_path=save_path_to_fine_tuned_model,\n",
    "        inference_config=no_preprocessing_inference_config,\n",
    "        ignore_pretraining_limits=True,\n",
    "    )\n",
    "\n",
    "    clf_finetuned = model_finetuned.fit(X_train, y_train)\n",
    "    acc_score = accuracy_score(y_test, clf_finetuned.predict(X_test))\n",
    "    print(\"accuracy_score (Finetuned):\", acc_score)\n",
    "    accuracy_scores.append(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2ba9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.8304347826086957\n",
      "Std Accuracy: 0.01816225962974384\n"
     ]
    }
   ],
   "source": [
    "# get mean and std of accuracy scores\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Std Accuracy:\", std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dfce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
