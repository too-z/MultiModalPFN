{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8292ed9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wall/anaconda3/envs/mmpfn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/wall/works/research/MultiModalPFN/mmpfn/models/dino_v2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os \n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from mmpfn.datasets.salary import SalaryDataset\n",
    "\n",
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, root_mean_squared_error\n",
    "\n",
    "from mmpfn.models.tabpfn_v2 import TabPFNClassifier\n",
    "from mmpfn.models.dino_v2.models.vision_transformer import vit_base\n",
    "from mmpfn.models.tabpfn_v2.constants import ModelInterfaceConfig\n",
    "from mmpfn.models.tabpfn_v2.preprocessing import PreprocessorConfig\n",
    "from mmpfn.scripts_finetune.finetune_tabpfn_main import fine_tune_tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd5c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getenv('HOME'), \"works/research/MultiModalPFN/mmpfn/data/salary\")\n",
    "dataset = SalaryDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0144ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 21:57:58,262] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   3%|▎         | 3/100 [00:01<01:06,  1.46it/s, Best Val. Loss=1.23, Best Val. Score=-1.23, Training Loss=1.23, Val. Loss=1.23, Patience=49, Utilization=0, Grad Norm=3.05][2025-09-17 21:57:59,482] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  40%|████      | 40/100 [00:24<00:38,  1.58it/s, Best Val. Loss=1.22, Best Val. Score=-1.22, Training Loss=1.18, Val. Loss=1.22, Patience=13, Utilization=0, Grad Norm=4.65][2025-09-17 21:58:23,111] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:04,  1.55it/s, Best Val. Loss=1.21, Best Val. Score=-1.21, Training Loss=1.2, Val. Loss=1.21, Patience=-47, Utilization=0, Grad Norm=4.65]                          \n",
      "[2025-09-17 21:59:01,971] INFO - Initial Validation Loss: 1.2310269125488225 Best Validation Loss: 1.2095900771352293 Total Steps: 101 Best Step: 96 Total Time Spent: 65.71284198760986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.45729166666666665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 21:59:03,646] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   2%|▏         | 2/100 [00:00<00:54,  1.80it/s, Best Val. Loss=1.21, Best Val. Score=-1.21, Training Loss=1.28, Val. Loss=1.21, Patience=50, Utilization=0, Grad Norm=nan][2025-09-17 21:59:04,172] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  33%|███▎      | 33/100 [00:20<00:44,  1.51it/s, Best Val. Loss=1.2, Best Val. Score=-1.2, Training Loss=1.2, Val. Loss=1.21, Patience=20, Utilization=0, Grad Norm=3.47]   [2025-09-17 21:59:23,991] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:03,  1.57it/s, Best Val. Loss=1.2, Best Val. Score=-1.2, Training Loss=1.15, Val. Loss=1.2, Patience=-47, Utilization=0, Grad Norm=7.8]                          \n",
      "[2025-09-17 22:00:06,765] INFO - Initial Validation Loss: 1.2081275681665542 Best Validation Loss: 1.2004059722804858 Total Steps: 101 Best Step: 73 Total Time Spent: 63.96515393257141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 22:00:08,544] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   2%|▏         | 2/100 [00:00<00:56,  1.73it/s, Best Val. Loss=1.25, Best Val. Score=-1.25, Training Loss=1.2, Val. Loss=1.25, Patience=50, Utilization=0, Grad Norm=nan][2025-09-17 22:00:09,052] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  37%|███▋      | 37/100 [00:22<00:41,  1.53it/s, Best Val. Loss=1.25, Best Val. Score=-1.25, Training Loss=1.23, Val. Loss=1.25, Patience=16, Utilization=0, Grad Norm=3.97][2025-09-17 22:00:31,113] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:03,  1.58it/s, Best Val. Loss=1.25, Best Val. Score=-1.25, Training Loss=1.12, Val. Loss=1.26, Patience=-47, Utilization=0, Grad Norm=5.02]                         \n",
      "[2025-09-17 22:01:11,098] INFO - Initial Validation Loss: 1.2534227256683133 Best Validation Loss: 1.2488374221986729 Total Steps: 101 Best Step: 46 Total Time Spent: 63.44593906402588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.4427083333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 22:01:12,772] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   3%|▎         | 3/100 [00:01<00:55,  1.74it/s, Best Val. Loss=1.26, Best Val. Score=-1.26, Training Loss=1.25, Val. Loss=1.26, Patience=49, Utilization=0, Grad Norm=4.33][2025-09-17 22:01:13,863] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  36%|███▌      | 36/100 [00:22<00:40,  1.59it/s, Best Val. Loss=1.26, Best Val. Score=-1.26, Training Loss=1.2, Val. Loss=1.26, Patience=17, Utilization=0, Grad Norm=4.94] [2025-09-17 22:01:35,042] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:03,  1.58it/s, Best Val. Loss=1.26, Best Val. Score=-1.26, Training Loss=1.13, Val. Loss=1.27, Patience=-47, Utilization=0, Grad Norm=6]                            \n",
      "[2025-09-17 22:02:15,479] INFO - Initial Validation Loss: 1.268140731811022 Best Validation Loss: 1.258111230360601 Total Steps: 101 Best Step: 22 Total Time Spent: 63.543938875198364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.4479166666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 22:02:17,070] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   9%|▉         | 9/100 [00:05<01:01,  1.47it/s, Best Val. Loss=1.22, Best Val. Score=-1.22, Training Loss=1.19, Val. Loss=1.22, Patience=43, Utilization=0, Grad Norm=3.6] [2025-09-17 22:02:22,369] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  55%|█████▌    | 55/100 [00:35<00:28,  1.58it/s, Best Val. Loss=1.21, Best Val. Score=-1.21, Training Loss=1.17, Val. Loss=1.21, Patience=-2, Utilization=0, Grad Norm=4.12][2025-09-17 22:02:52,233] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:04,  1.56it/s, Best Val. Loss=1.21, Best Val. Score=-1.21, Training Loss=1.14, Val. Loss=1.21, Patience=-47, Utilization=0, Grad Norm=4.21]                         \n",
      "[2025-09-17 22:03:20,780] INFO - Initial Validation Loss: 1.2276401138590871 Best Validation Loss: 1.206946831722423 Total Steps: 101 Best Step: 64 Total Time Spent: 64.54635047912598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.45208333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 22:03:22,422] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   9%|▉         | 9/100 [00:05<00:58,  1.55it/s, Best Val. Loss=1.24, Best Val. Score=-1.24, Training Loss=1.26, Val. Loss=1.24, Patience=43, Utilization=0, Grad Norm=3.49][2025-09-17 22:03:27,503] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  35%|███▌      | 35/100 [00:21<00:41,  1.56it/s, Best Val. Loss=1.24, Best Val. Score=-1.24, Training Loss=1.27, Val. Loss=1.24, Patience=18, Utilization=0, Grad Norm=3.97][2025-09-17 22:03:43,701] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:02,  1.59it/s, Best Val. Loss=1.23, Best Val. Score=-1.23, Training Loss=1.15, Val. Loss=1.23, Patience=-47, Utilization=0, Grad Norm=4.5]                          \n",
      "[2025-09-17 22:04:24,726] INFO - Initial Validation Loss: 1.2468502668840875 Best Validation Loss: 1.2271878307264217 Total Steps: 101 Best Step: 82 Total Time Spent: 63.11437249183655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 22:04:26,185] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   3%|▎         | 3/100 [00:00<00:48,  2.01it/s, Best Val. Loss=1.28, Best Val. Score=-1.28, Training Loss=1.21, Val. Loss=1.28, Patience=49, Utilization=0, Grad Norm=2.76][2025-09-17 22:04:27,215] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  67%|██████▋   | 67/100 [00:41<00:19,  1.65it/s, Best Val. Loss=1.26, Best Val. Score=-1.26, Training Loss=1.23, Val. Loss=1.26, Patience=-14, Utilization=0, Grad Norm=4.21][2025-09-17 22:05:07,547] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps: 101it [01:02,  1.60it/s, Best Val. Loss=1.26, Best Val. Score=-1.26, Training Loss=1.24, Val. Loss=1.27, Patience=-47, Utilization=0, Grad Norm=5.37]                         \n",
      "[2025-09-17 22:05:28,135] INFO - Initial Validation Loss: 1.276273155536029 Best Validation Loss: 1.2613753511672063 Total Steps: 101 Best Step: 89 Total Time Spent: 62.64948844909668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score (Finetuned): 0.46458333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Steps:   1%|          | 1/100 [00:00<?, ?it/s][2025-09-17 22:05:29,774] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:   2%|▏         | 2/100 [00:00<00:56,  1.74it/s, Best Val. Loss=1.23, Best Val. Score=-1.23, Training Loss=1.3, Val. Loss=1.23, Patience=50, Utilization=0, Grad Norm=nan][2025-09-17 22:05:30,268] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  23%|██▎       | 23/100 [00:13<00:48,  1.60it/s, Best Val. Loss=1.22, Best Val. Score=-1.22, Training Loss=1.17, Val. Loss=1.22, Patience=30, Utilization=0, Grad Norm=3.28][2025-09-17 22:05:43,673] INFO - \n",
      "Optimizer step skipped due to NaNs/infs in grad scaling.\n",
      "Fine-tuning Steps:  86%|████████▌ | 86/100 [00:53<00:08,  1.59it/s, Best Val. Loss=1.22, Best Val. Score=-1.22, Training Loss=1.2, Val. Loss=1.23, Patience=-32, Utilization=0, Grad Norm=5.31] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     26\u001b[0m save_path_to_fine_tuned_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./finetuned_tabpfn_pad_ufes_20.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mfine_tune_tabpfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# path_to_base_model=\"auto\",\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path_to_fine_tuned_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path_to_fine_tuned_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Finetuning HPs\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetuning_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Input Data\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_features_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use \"cpu\" if you don't have a GPU\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulticlass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optional\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_training_curve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shows a final report after finetuning.\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shows all logs, higher values shows less\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# disables preprocessing at inference time to match fine-tuning\u001b[39;00m\n\u001b[1;32m     47\u001b[0m no_preprocessing_inference_config \u001b[38;5;241m=\u001b[39m ModelInterfaceConfig(\n\u001b[1;32m     48\u001b[0m     FINGERPRINT_FEATURE\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m     PREPROCESS_TRANSFORMS\u001b[38;5;241m=\u001b[39m[PreprocessorConfig(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     50\u001b[0m )\n",
      "File \u001b[0;32m~/works/research/MultiModalPFN/mmpfn/scripts_finetune/finetune_tabpfn_main.py:365\u001b[0m, in \u001b[0;36mfine_tune_tabpfn\u001b[0;34m(path_to_base_model, save_path_to_fine_tuned_model, time_limit, finetuning_config, validation_metric, X_train, y_train, categorical_features_index, task_type, device, use_multiple_gpus, multiple_device_ids, X_val, y_val, random_seed, logger_level, show_training_curve)\u001b[0m\n\u001b[1;32m    363\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    364\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 365\u001b[0m step_results \u001b[38;5;241m=\u001b[39m \u001b[43m_fine_tune_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_X_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_X_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_y_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_y_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_forward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_forward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Updated by the loop\u001b[39;49;00m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_with_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_now\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_results\u001b[38;5;241m.\u001b[39moptimizer_step_skipped:\n\u001b[1;32m    382\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimizer step skipped due to NaNs/infs in grad scaling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/works/research/MultiModalPFN/mmpfn/scripts_finetune/finetune_tabpfn_main.py:636\u001b[0m, in \u001b[0;36m_fine_tune_step\u001b[0;34m(batch_X_train, batch_X_test, batch_y_train, batch_y_test, device, model, optimizer, model_forward_fn, loss_fn, scaler, step_with_update, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_with_update:\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# Clip grads\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    632\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# Step optimizer and scaler\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     org_scale \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "for seed in range(10):\n",
    "    torch.manual_seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # print(f\"Finetuning with seed: {seed}\")\n",
    "    \n",
    "    train_len = int(len(dataset) * 0.8)\n",
    "    test_len = len(dataset) - train_len\n",
    "\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "    X_train = train_dataset.dataset.x[train_dataset.indices]\n",
    "    y_train = train_dataset.dataset.y[train_dataset.indices]\n",
    "    X_test = test_dataset.dataset.x[test_dataset.indices]\n",
    "    y_test = test_dataset.dataset.y[test_dataset.indices]\n",
    "\n",
    "    for i in range(X_train.shape[1]):\n",
    "        col = X_train[:, i]\n",
    "        col[np.isnan(col)] = np.nanmin(col) - 1\n",
    "    for i in range(X_test.shape[1]):\n",
    "        col = X_test[:, i]\n",
    "        col[np.isnan(col)] = np.nanmin(col) - 1\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    save_path_to_fine_tuned_model = \"./finetuned_tabpfn_pad_ufes_20.ckpt\"\n",
    "    \n",
    "    fine_tune_tabpfn(\n",
    "        # path_to_base_model=\"auto\",\n",
    "        save_path_to_fine_tuned_model=save_path_to_fine_tuned_model,\n",
    "        # Finetuning HPs\n",
    "        time_limit=60,\n",
    "        finetuning_config={\"learning_rate\": 0.00001, \"batch_size\": 1, \"max_steps\": 100},\n",
    "        validation_metric=\"log_loss\",\n",
    "        # Input Data\n",
    "        X_train=pd.DataFrame(X_train),\n",
    "        y_train=pd.Series(y_train),\n",
    "        categorical_features_index=None,\n",
    "        device=\"cuda\",  # use \"cpu\" if you don't have a GPU\n",
    "        task_type=\"multiclass\",\n",
    "        # Optional\n",
    "        show_training_curve=False,  # Shows a final report after finetuning.\n",
    "        logger_level=0,  # Shows all logs, higher values shows less\n",
    "    )\n",
    "\n",
    "    # disables preprocessing at inference time to match fine-tuning\n",
    "    no_preprocessing_inference_config = ModelInterfaceConfig(\n",
    "        FINGERPRINT_FEATURE=False,\n",
    "        PREPROCESS_TRANSFORMS=[PreprocessorConfig(name='none')]\n",
    "    )\n",
    "\n",
    "    # Evaluate on Test Data\n",
    "    model_finetuned = TabPFNClassifier(\n",
    "        model_path=save_path_to_fine_tuned_model,\n",
    "        inference_config=no_preprocessing_inference_config,\n",
    "        ignore_pretraining_limits=True,\n",
    "    )\n",
    "\n",
    "    clf_finetuned = model_finetuned.fit(X_train, y_train)\n",
    "    acc_score = accuracy_score(y_test, clf_finetuned.predict(X_test))\n",
    "    print(\"accuracy_score (Finetuned):\", acc_score)\n",
    "    accuracy_scores.append(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ba9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.4552083333333333\n",
      "Std Accuracy: 0.009512875047359058\n"
     ]
    }
   ],
   "source": [
    "# get mean and std of accuracy scores\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Std Accuracy:\", std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c2b684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4548611111111111)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.45, 0.45729166666666665, 0.45729166666666665])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78689387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.003437324630767927)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([0.45, 0.45729166666666665, 0.45729166666666665])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b194255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.45729166666666665, 0.45, 0.4427083333333333, 0.4479166666666667, 0.45208333333333334])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0f2748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.45)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.45729166666666665, 0.45, 0.4427083333333333, 0.4479166666666667, 0.45208333333333334])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
